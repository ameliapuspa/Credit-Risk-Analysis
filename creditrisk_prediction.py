# -*- coding: utf-8 -*-
"""CreditRisk_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yfylXWKpPutkV6uOGG5mspL5c5VslvyO

Import Library
"""

import os
from google.colab import drive

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import datetime

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import tensorflow as tf

from sklearn.metrics import confusion_matrix, classification_report

"""Load Dataset"""

drive.mount('/content/gdrive')

data = '/content/gdrive/MyDrive/PBI ID X Partners/Final Task/loan_data_2007_2014.csv'

df = pd.read_csv(data, low_memory=False)
df.head()

"""# Data Understanding"""

#How many rows and columns in the dataset?
df.shape

#General information of the dataset
df.info()

#Checking for missing values
df.isna().sum()

#Menghapus kolom yang hanya berisi nilai NaN.
df = df.drop(columns = ['Unnamed: 0'])
df = df.dropna(axis=1, how='all')

df.info()

#Menghapus unused columns
dropped = ['id', 'member_id', 'issue_d', 'url', 'delinq_2yrs', 'inq_last_6mths',
    'mths_since_last_delinq', 'mths_since_last_record', 'out_prncp',
    'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',
    'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
    'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',
    'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'tot_coll_amt',
    'tot_cur_bal', 'total_rev_hi_lim']

df = df.drop(dropped, axis=1)

"""# Exploratory Data Analysis (EDA)"""

df.info()

#Ringkasan statistik deskriptif dari dataset
df.describe()

#Melihat informasi tentang kolom-kolom yang memiliki tipe data string
df.select_dtypes(include=object).info()

#Menghitung jumlah nilai unik dalam setiap kolom yang memiliki tipe data string
df.select_dtypes(include=object).nunique()

#Melihat informasi tentang kolom-kolom yang memiliki tipe data numerik
df.select_dtypes(include='number').info()

#Menghitung jumlah nilai unik dalam setiap kolom yang memiliki tipe data numerik.
df.select_dtypes(include='number').nunique()

#Mendapatkan nilai unik dari setiap kolom dalam DataFrame
unique_values = df.apply(lambda x: x.unique())
unique_values

#Menghitung frekuensi kemunculan setiap nilai unik dalam kolom loan_status
df['loan_status'].value_counts()

"""**Loan Status**

Untuk mempermudah proses pelatihan model dan meningkatkan akurasi prediksi, variabel 'loan_status' diklasifikasikan menjadi dua kategori:


*   0: Bad (pinjaman dengan tunggakan lebih dari 30 hari atau gagal bayar)
*   1: Good (pinjaman dengan pembayaran tepat waktu atau dengan pengecualian tertentu).

Dengan menggabungkan kelas `Fully Paid`, `Does not meet the credit policy. Status:Fully Paid`, `In Grace Period`, dan `Late (16 - 30 days)` menjadi satu kategori yang positif, serta menggabungkan kelas `Charged Off`, `Default`, `Does not meet the credit policy. Status:Charged Off`, `Late (31-120 days)` menjadi satu kategori yang negatif.
Pengurangan jumlah kategori ini memungkinkan model untuk lebih fokus pada faktor-faktor utama yang memengaruhi kelayakan kredit, sekaligus mengurangi risiko overfitting dan meningkatkan performa prediksi secara keseluruhan.

Kategori status kredit `Current` dikecualikan dari analisis karena masih dalam proses dan tidak dapat diprediksi. Sementara itu, kategori `Late (16-30 days)` dianggap sebagai pinjaman baik karena umumnya pinjaman tidak digolongkan buruk sebelum tunggakan melebihi 30 hari.



"""

good_classes = ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid', 'In Grace Period', 'Late (16 - 30 days)']
df_good = df[df['loan_status'].isin(good_classes)]
df_good.head()

df = df.loc[~df['loan_status'].isin(['Current'])].reset_index(drop=True)

df_good['loan_status'].value_counts().sum()

df['loan_status'] = np.where(df['loan_status'].isin(["Fully Paid",
                                                     "Does not meet the credit policy. Status:Fully Paid",
                                                     "In Grace Period", "Late (16 - 30 days)"
                                                     ]),
                             "Good",
                             df['loan_status'])

df['loan_status'] = np.where(df['loan_status'].isin(["Charged Off",
                                                     "Default",
                                                     "Does not meet the credit policy. Status:Charged Off",
                                                     "Late (31-120 days)"
                                                     ]),
                             "Bad",
                             df['loan_status'])

df = df[df['loan_status'].isin(["Good", "Bad"])]

plt.figure(figsize=(15, 5))

count = df['loan_status'].value_counts()
ax = sns.barplot(x=count.index, y=count.values, hue=count.index, legend=False)

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Loan Status Keseluruhan')
plt.xlabel('Loan Status')
plt.ylabel('Total')

plt.show()

df['loan_status'].value_counts()

"""# Data Analysis
*   Univariate Analysis


"""

df['application_type'].unique()

df = df.drop(['application_type'], axis=1)

df['policy_code'].unique()

df = df.drop(['policy_code'], axis=1)

df['emp_title'].head(10)

df['emp_title'].isna().sum()

"""Berdasarkan hasil analisis pada tahap Deskripsi Variabel, variabel `emp_title` memiliki jumlah nilai unik yang sangat banyak. Selain itu, variabel ini juga mengandung *NaN* dalam jumlah besar, yaitu sebanyak 13.523. Banyaknya kategori dan nilai *NaN* tersebut berpotensi menimbulkan kebingungan selama proses pelatihan model. Oleh karena itu, kolom `emp_title` akan dihapus."""

df = df.drop(['emp_title'], axis=1)

df['emp_length'].unique()

#Cleaning emp_length values
df['emp_length'] = df['emp_length'].str.replace('\+ years', '')
df['emp_length'] = df['emp_length'].str.replace('< 1 year', str(0))
df['emp_length'] = df['emp_length'].str.replace(' years', '')
df['emp_length'] = df['emp_length'].str.replace(' year', '')
df['emp_length'] = df['emp_length'].str.replace('+', '')
df['emp_length'] = df['emp_length'].astype(float)

#Looking at values in other columns
cat = df.select_dtypes (include= ['object'])

for col in cat.columns.tolist():
    print(df[col].value_counts()[:20])
    print('\n')

#Dominated by a single value
df.drop('pymnt_plan', axis=1, inplace=True)

#Melakukan pengecekkan pada kolom earliest_cr_line
df['earliest_cr_line'].head(10)

#Mengubah kolom earliest_cr_line menjadi tipe data datetime. Kemudian menghasilkan deskripsi statistik dari kolom tersebut
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['earliest_cr_line'].describe()

!pip install --upgrade pandas

import pandas as pd
print(pd.__version__)

#Memperbaiki tanggal yang disimpan dalam format datetime
def fix_date(x):
    if pd.isnull(x):
        return np.nan
    if x.year > 2030:
        year = x.year - 100
    else:
        year = x.year
    return datetime.date(year, x.month, x.day)

df['earliest_cr_line'] = df['earliest_cr_line'].apply(fix_date)

#Memberikan ringkasan statistik terkait kolom dengan tipe data datetime
df['earliest_cr_line'].describe()

#Format data dalam kolom earliest_cr_line dari datetime menjadi hanya tahun.
df['earliest_cr_line'] = pd.DatetimeIndex(df['earliest_cr_line']).year

#Menghitung jumlah nilai unik dalam kolom earliest_cr_line.
df['earliest_cr_line'].nunique()

#Menampilkan distribusi tanggal pada kolom earliest_cr_line.
sns.histplot(data=df, x="earliest_cr_line", hue="loan_status", bins=65)

plt.show()

#Memberikan ringkasan statistik deskriptif pada kolom loan_amnt.
df['loan_amnt'].describe()

#Membuat histogram untuk setiap kolom dalam DataFrame terhadap data yang bersifat numerik.
df.hist(bins=50, figsize=(20, 15))
plt.show()

"""

*   Multivariate Analysis

"""

#Melakukan pengecekkan pada kolom desc, purpose, and title
df[['desc', 'purpose', 'title']].head(10)

df['desc_is_null'] = df['desc'].isnull()

df['desc_is_null'].value_counts()

#Menghapus kolom desc
df = df.drop('desc', axis=1)

df = df.drop('desc_is_null', axis=1)

#Menghitung jumlah nilai unik yang muncul dalam kolom title dan purpose. Kemudian mengambil sepuluh kombinasi nilai unik yang paling umum

df[['title', 'purpose']].value_counts()[:10]

"""Berdasarkan hasil di atas, dapat disimpulkan bahwa variabel `purpose` secara langsung berasal dari `title`. Dengan demikian, `title` dianggap sebagai variabel yang redundan karena telah direpresentasikan dalam bentuk purpose."""

#Menghapus kolom title
df = df.drop('title', axis=1)

#Melakukan pengecekkan pada kolom zip_code dan addr_state
df[['zip_code', 'addr_state']].head(5)

#Menghitung jumlah kemunculan (count) kombinasi nilai dari dua kolom, yaitu zip_code dan addr_state

df[['zip_code', 'addr_state']].value_counts()[:20]

# Filter data untuk top 20 states
top_states = df['addr_state'].value_counts()[:20].index
filtered_df = df[df['addr_state'].isin(top_states)]

# Membuat pivot table untuk distribusi loan_status terhadap addr_state
data_plot = filtered_df.pivot_table(index='addr_state', columns='loan_status', aggfunc='size', fill_value=0)

# Menambahkan kolom total dan mengurutkan
data_plot['Total'] = data_plot.sum(axis=1)
data_plot = data_plot.sort_values(by='Total', ascending=False)
data_plot = data_plot.drop(columns='Total')  # Hapus kolom Total setelah sorting

# Plot stacked bar chart
data_plot.plot(kind='bar', stacked=True, colormap='Set3', figsize=(12, 6))

# Menambahkan detail plot
plt.title('Top 20 States: Loan Status Distribution', fontsize=14)
plt.xlabel('States', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.legend(title='Loan Status')
plt.tight_layout()

plt.show()

state_bad_loan_ratio = df.groupby('addr_state')['loan_status'].value_counts(normalize=True).unstack()
# Access columns by name 'Bad' and 'Good' instead of index 0 and 1
state_bad_loan_ratio['bad_loan_ratio'] = state_bad_loan_ratio['Bad'] / (state_bad_loan_ratio['Good'] + state_bad_loan_ratio['Bad'])

state_bad_loan_ratio = state_bad_loan_ratio.sort_values(by='bad_loan_ratio', ascending=False)

print(state_bad_loan_ratio)

print("\nTop 5 highest bad loan ratio states:")
print(state_bad_loan_ratio['bad_loan_ratio'].head(5))

print("\nTop 5 lowest bad loan ratio states:")
print(state_bad_loan_ratio['bad_loan_ratio'].tail(5))

#Menghapus kolom zip_code
df = df.drop('zip_code', axis=1)

"""grade dan sub grade"""

#Menampilkan nilai unik yang terdapat dalam kolom grade dan sub grade
print(df['grade'].unique())
print(df['sub_grade'].unique())

fig, axs = plt.subplots(ncols=2, figsize=(20, 5))
colors_grade = ['#84a59d', '#f28482']
pd.crosstab(df['grade'], df['loan_status']).plot(kind='bar', ax=axs[0], color=colors_grade)

colors_subgrade = ['#84a59d', '#f28482']
pd.crosstab(df['sub_grade'], df['loan_status']).plot(kind='bar', stacked=True, ax=axs[1], color=colors_subgrade)

plt.show()

#Membuat tabel silang antara kolom grade dan loan_status dan menormalisasi berdasarkan baris ('index')

pd.crosstab(df['grade'], df['loan_status'], normalize='index').round(2)

#Membuat tabel silang antara kolom sub_grade dan loan_status dan menormalisasi berdasarkan baris ('index')

pd.crosstab(df['sub_grade'], df['loan_status'], normalize='index').round(2)

"""Dari informasi di atas, dapat disimpulkan bahwa setiap `sub_grade` sudah merepresentasikan `grade`. Oleh karena itu, variabel `grade` dapat dihapus untuk mengurangi redundansi data."""

#Menghapus kolom grade
df = df.drop('grade', axis=1)

df.info()

#Membuat sebuah plot heatmap yang menampilkan matriks korelasi antar variabel.
plt.subplots(figsize=(20, 10))
sns.heatmap(df.corr(numeric_only=True), vmin=-1, vmax=1, annot=True, cmap="YlGn")

plt.show()

"""Variabel `loan_amnt`,`funded_amnt`, dan `funded_amnt_inv` menunjukkan korelasi sempurna, yang mengindikasikan bahwa ketiganya bersifat redundan. Oleh karena itu, diputuskan untuk hanya menggunakan variabel `loan_amnt`, karena berdasarkan Data Dictionary, perubahan jumlah pinjaman tercermin pada variabel tersebut."""

#Menghapus variabel yang redundan dan tidak relevan
df = df.drop(['funded_amnt', 'funded_amnt_inv'], axis=1)

"""**loan_amnt dan installment**

Terdapat korelasi yang signifikan antara variabel `loan_amnt` dan `installment`. Oleh karena itu, kedua variabel tersebut akan dieksplorasi guna menentukan apakah terdapat duplikasi informasi di antara keduanya serta memahami hubungan keduanya dengan variabel target.
"""

#Membuat subplot masing-masing variabel dengan dua boxplot secara horizontal.
fig, axs = plt.subplots(ncols=2, figsize=(25, 10))
sns.boxplot(x='loan_status', y='loan_amnt', data=df, ax=axs[0], hue='loan_status', legend=False)
sns.boxplot(x='loan_status', y='installment', data=df, ax=axs[1], hue='loan_status', legend=False)

plt.show()

#Membuat subplot dengan dua histogram secara horizontal
fig, axs = plt.subplots(ncols=2, figsize=(25, 10))
sns.histplot(data=df, x="loan_amnt", hue="loan_status", bins=11, ax=axs[0])
sns.histplot(data=df, x="installment", hue="loan_status", bins=50, ax=axs[1])

plt.show()
plt.show()

"""Hasil observasi terhadap distribusi variabel `loan_amnt` dan `installment `menunjukkan tidak adanya redundansi informasi. Hal ini karena `installment` merepresentasikan jumlah pembayaran bulanan yang dihitung berdasarkan loan_amnt, term, dan int_rate. Oleh karena itu, kedua variabel tersebut tetap relevan untuk digunakan, guna meningkatkan interpretabilitas model. Selain itu, `installment` memiliki hubungan erat dengan karakteristik pinjaman, sehingga dapat menjadi variabel penting dalam eksperimen pemodelan untuk memaksimalkan kualitas prediksi.

# Data Preparation


*   Handle Missing Data
    
    Melakukan pemeriksaan pada setiap kolom dalam DataFrame. Jika ditemukan nilai yang hilang (NaN), kode akan menampilkan nama kolom, jumlah nilai yang hilang, serta persentasenya terhadap total data.
"""

for column in df.columns:
    if df[column].isna().sum() != 0:
        missing = df[column].isna().sum()
        missing_prob = (missing / df.shape[0]) * 100
        print(f"{column}: {missing}({missing_prob:.2f}%)")

#Membuat tabel silang (crosstab) antara kolom emp_length dan loan_status
pd.crosstab(df['emp_length'], df['loan_status'], normalize='index').round(2)

"""Proporsi `loan_status` di antara kategori `emp_length` terlihat sangat mirip. Namun, sebelum memutuskan apakah kolom tersebut perlu dihapus, penting untuk terlebih dahulu memeriksa komposisi `emp_length` yang memiliki nilai NaN."""

df[df['emp_length'].isnull()]['loan_status'].value_counts()

"""Berdasarkan informasi di atas, `emp_length` dengan nilai NaN masih mengandung banyak data penting dari klien, baik kategori Good maupun Bad. Menghapus baris dengan nilai NaN pada `emp_length` akan menghilangkan sejumlah besar informasi berharga tersebut. Oleh karena itu, lebih tepat untuk menghapus kolom `emp_length` secara keseluruhan."""

df = df.drop('emp_length', axis=1)

"""Melihat komposisi dari annual_inc, earliest_cr_line, open_acc, pub_rec, revol_util, total_acc, acc_now_delinq yang memiliki nilai NaN."""

columns_to_check = ['annual_inc', 'earliest_cr_line', 'open_acc', 'pub_rec', 'revol_util', 'total_acc', 'acc_now_delinq']

for column in columns_to_check:
    print(f"Jumlah dari kolom {column} yang memiliki nilai NaN:")
    print(df[df[column].isnull()]['loan_status'].value_counts())
    print("")

"""Berdasarkan informasi mengenai komposisi data, jumlah nilai NaN pada kolom `annual_inc`, `earliest_cr_line`, `open_acc`, `pub_rec`, `revol_util`, `total_acc`, dan `acc_now_delinq` tergolong kecil. Oleh karena itu, diputuskan untuk menghapus baris-baris yang mengandung nilai NaN pada kolom-kolom tersebut."""

cols = ['annual_inc', 'earliest_cr_line', 'open_acc', 'pub_rec', 'revol_util', 'total_acc', 'acc_now_delinq']
for col in cols:
  df = df[df[col].notnull()]

"""


*   Convert to Numeric Variable


"""

df.select_dtypes(include=np.number).info()

#Mengubah tipe data kolom tertentu dalam DataFrame df menjadi tipe data yang sesuai dengan kebutuhan pemodelan.

col_to_int = ['earliest_cr_line', 'open_acc', 'pub_rec',
              'total_acc', 'acc_now_delinq']
col_to_float = ['revol_bal', 'loan_amnt']

df[col_to_int] = df[col_to_int].astype(int)
df[col_to_float] = df[col_to_float].astype(float)

"""

*   Remove Outliers



"""

cols = df.select_dtypes(float).columns
df_sub = df.loc[:, cols]

"""Menghasilkan boxplot yang menunjukkan distribusi batas atas dan batas bawah yang digunakan untuk mendeteksi outlier dalam DataFrame."""

Q1 = df_sub.quantile(0.25)
Q3 = df_sub.quantile(0.75)
IQR=Q3-Q1

outliers = ~((df_sub < (Q1 - 1.5 * IQR)) | (df_sub > (Q3 + 1.5 * IQR)))

plt.figure(figsize=(10, 5))
sns.boxplot(data=outliers)
plt.title('Deteksi Outlier')
plt.ylabel('Batas Atas dan Bawah')
plt.tight_layout()
plt.show()

"""Membersihkan outlier dari DataFrame dengan mengganti nilai-nilai outlier dengan NaN dan menghapus baris-baris yang mengandung NaN dalam kolom-kolom yang dipilih."""

df.loc[:, cols] = df_sub.where(outliers, np.nan)
df.dropna(subset=cols, inplace=True)

#Menampilkan visualisasi data menggunakan boxplot setelah menghapus outlier.
plt.figure(figsize=(10, 5))
sns.boxplot(data=df[cols])
plt.title('Boxplot Setelah Menghapus Outlier')
plt.ylabel('Nilai')
plt.tight_layout()
plt.show()

"""

*   One-Hot Encoding

"""

#Membuat sebuah objek LabelEncoder yang dapat digunakan untuk melakukan encoding pada label kelas.
le = LabelEncoder()

#Melakukan encoding pada kolom loan_status
df.loan_status = le.fit_transform(df.loan_status)

#Mencetak daftar kelas unik yang telah dipelajari oleh objek LabelEncoder
print(le.classes_)

#Mengubah label kelas dalam bentuk teks menjadi representasi numerik menggunakan objek LabelEncoder.
print(le.transform(['Bad', 'Good']))

""" Variabel kategorikal dalam DataFrame diubah menjadi representasi numerik yang dapat digunakan untuk analisis lebih lanjut, terutama dalam konteks pemodelan data."""

#Memilih kolom-kolom dalam DataFrame yang memiliki tipe data objek
df.select_dtypes(include=object).head()

print(df['term'].unique())

#Mengubah kolom 'term' dari representasi teks menjadi representasi numerik (dalam bulan)
df['term'] = np.where(df['term'] == " 36 months", 36,
             np.where(df['term'] == " 60 months", 60,
             df['term']))
df['term'] = df['term'].astype(int)
print(df['term'].unique())

"""**Transformasi**"""

#Mendapatkan daftar kolom-kolom dalam DataFrame yang memiliki tipe data objek.
categories = df.select_dtypes(include=[object]).columns.tolist()

#Mendapatkan daftar kolom-kolom dalam DataFrame yang memiliki tipe data numerik.
numerics = df.drop(['loan_status'], axis=1).select_dtypes(include=np.number).columns.tolist()

#Menginisialisasi objek dari kelas OneHotEncoder dan MinMaxScaler.
ohe = OneHotEncoder(sparse_output=False)
scaler = MinMaxScaler()

ct = ColumnTransformer(
    [("onehotencoder", ohe, categories),
     ("minmaxscaler", scaler, numerics)],
    remainder = 'passthrough'
)

ct.set_output(transform='pandas')
processed_df = ct.fit_transform(df.drop(['loan_status'], axis=1))

"""Menggabungkan DataFrame yang telah diproses processed_df dengan kolom loan_status dari DataFrame asli df"""

processed_df = pd.concat([processed_df, df['loan_status']], axis=1)

#Menghitung jumlah nilai unik dari variabel target loan_status dalam DataFrame processed_df
processed_df.loan_status.value_counts()

"""

* Splitting the dataset into the Training set and Test set

    Membagi DataFrame processed_df menjadi dua subset dengan proporsi sebesar 20% untuk data train, sedangkan 80% sisanya dialokasikan untuk data test.
"""

train, test = train_test_split(processed_df, test_size=0.2, random_state=42)

"""Membagi data latih (train) dan data uji (test) menjadi dua bagian terpisah yang terdiri dari fitur (X_train dan X_test) dan variabel target (y_train dan y_test)."""

X_train, y_train = train.drop('loan_status', axis=1), train.loan_status
X_test, y_test = test.drop('loan_status', axis=1), test.loan_status

"""# Modelling"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=[X_train.shape[1]]),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    x=X_train,
    y=y_train,
    epochs=15,
    batch_size=256,
    validation_data=(X_test, y_test)
)

# Plot the chart for accuracy and loss on both training and validation
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

key = ['RandomForestClassifier',
       'GradientBoostingClassifier',
       'LogisticRegression',
       'NeuralNetwork']

value = [RandomForestClassifier(n_estimators=100, random_state=0, max_depth=None),
         GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
         LogisticRegression(max_iter=1000),
         model]

models = dict(zip(key, value))

accuracy = pd.DataFrame(columns=['Accuracy Score'], index=key)

for name, algo in models.items():

  model = algo
  model.fit(X_train,y_train)

  if name == 'NeuralNetwork':
    predict = (model.predict(X_test) > 0.5).astype("int32")
  else:
    predict = model.predict(X_test)

  accuracy.loc[name] = accuracy_score(y_test, predict)

accuracy

plt.figure(figsize=(10, 5))

ax = sns.barplot(data=accuracy, x="Accuracy Score", y=accuracy.index, hue=accuracy.index, legend=False, orient='y')

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Perbandingan Skor Akurasi Setiap Model')
plt.xlabel('Model')
plt.ylabel('Accuracy Score')
plt.xlim(0, 1)

plt.show()

"""# Evaluation


*   Confusion Matrix


"""

confusion_matrices = {}

for name, algo in models.items():
    confusion_matrices[name] = confusion_matrix(y_test, predict)

plt.figure(figsize=(18, 15))
for i, (name, matrix) in enumerate(confusion_matrices.items(), 1):
    plt.subplot(3, 3, i)
    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')
    plt.title(f'Confusion Matrix Algoritma {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
plt.tight_layout()
plt.show()

"""

*   Classification Report
"""

#Cek model performance menggunakan classification_report
classification_reports = {}

for name, algo in models.items():

    report = classification_report(y_test, predict)
    classification_reports[name] = report

for name, report in classification_reports.items():
    print(f"Classification Report Algoritma {name}:\n{report}\n")

"""# ROC Curve dan AUC"""

auc_scores = pd.DataFrame(columns=['AUC'], index=key)

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
plt.figure(figsize=(8, 6))

for name, algo in models.items():
    model = algo
    model.fit(X_train, y_train)

    if name == 'NeuralNetwork':
        predict = model.predict(X_test)
    else:
        predict = model.predict_proba(X_test)[:, 1]

    fpr, tpr, thresholds = roc_curve(y_test, predict)
    roc_auc = auc(fpr, tpr)

    auc_scores.loc[name] = roc_auc

    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")

plt.show()

auc_scores

plt.figure(figsize=(10, 5))

ax = sns.barplot(data=auc_scores, x="AUC", y=auc_scores.index, hue=auc_scores.index, palette='Set2', legend=False, orient='y')

for i, container in enumerate(ax.containers):
    ax.bar_label(container, fontsize=10)

ax.set_title('Perbandingan Skor AUC untuk Setiap Model')
plt.xlabel('Model')
plt.ylabel('AUC Score')
plt.xlim(0, 1)

plt.show()